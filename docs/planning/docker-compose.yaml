# docker-compose.phase1.yml - PromptCraft-Hybrid Phase 1 with Context7
# Enhanced MCP stack for immediate Journey 3 value + tiered search capability

version: '3.8'

services:
  # === Core Orchestration ===
  zen-mcp-server:
    build:
      context: ./zen-mcp-server
      dockerfile: Dockerfile
    container_name: zen-mcp
    ports:
      - "3000:3000"
    environment:
      - ZEN_SERVER_PORT=3000
      - ZEN_LOG_LEVEL=INFO
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT}
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY}
      - MCP_SERVERS=serena,filesystem,github,sequential-thinking,qdrant-memory,context7
      - CONTEXT7_API_KEY=${CONTEXT7_API_KEY}
      - ENABLE_SEARCH_TIER=true
    volumes:
      - ./config/zen-config.yaml:/app/config.yaml:ro
      - ./logs:/app/logs
    depends_on:
      - serena-mcp
      - filesystem-mcp
      - github-mcp
      - sequential-thinking-mcp
      - qdrant-memory-mcp
      - context7-mcp
    networks:
      - promptcraft
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3


  # === Semantic Code Analysis ===
  serena-mcp:
    build:
      context: ./mcp-servers/serena
      dockerfile: Dockerfile
    container_name: serena-mcp
    ports:
      - "8000:8000"
    environment:
      - MCP_PORT=8000
      - MCP_HOST=0.0.0.0
      - LOG_LEVEL=INFO
    volumes:
      - ./projects:/workspace:ro
      - ./config/serena-config.yml:/app/serena_config.yml:ro
      - lsp-cache:/tmp/lsp
    networks:
      - promptcraft
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # === Filesystem Access ===
  filesystem-mcp:
    build:
      context: ./mcp-servers/filesystem
      dockerfile: Dockerfile
    container_name: filesystem-mcp
    ports:
      - "8001:8001"
    environment:
      - MCP_PORT=8001
      - MCP_HOST=0.0.0.0
      - ALLOWED_DIRECTORIES=/workspace,/tmp
      - READ_ONLY_MODE=false
      - LOG_LEVEL=INFO
    volumes:
      - ./projects:/workspace:rw
      - ./temp:/tmp:rw
    networks:
      - promptcraft
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    security_opt:
      - no-new-privileges:true
      - seccomp:unconfined
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # === GitHub Integration ===
  github-mcp:
    build:
      context: ./mcp-servers/github
      dockerfile: Dockerfile
    container_name: github-mcp
    ports:
      - "8002:8002"
    environment:
      - GITHUB_TOKEN=${GITHUB_TOKEN}
      - GITHUB_API_URL=https://api.github.com
      - MCP_PORT=8002
      - MCP_HOST=0.0.0.0
      - LOG_LEVEL=INFO
      - RATE_LIMIT_REQUESTS_PER_HOUR=4000
    networks:
      - promptcraft
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # === Enhanced Reasoning ===
  sequential-thinking-mcp:
    build:
      context: ./mcp-servers/sequential-thinking
      dockerfile: Dockerfile
    container_name: sequential-thinking-mcp
    ports:
      - "8003:8003"
    environment:
      - MCP_PORT=8003
      - MCP_HOST=0.0.0.0
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT}
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY}
      - LOG_LEVEL=INFO
      - MAX_THINKING_STEPS=10
    networks:
      - promptcraft
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # === Qdrant Memory Helper ===
  qdrant-memory-mcp:
    build:
      context: ./mcp-servers/qdrant-memory
      dockerfile: Dockerfile
    container_name: qdrant-memory-mcp
    ports:
      - "8004:8004"
    environment:
      - MCP_PORT=8004
      - MCP_HOST=0.0.0.0
      - QDRANT_HOST=192.168.1.16
      - QDRANT_PORT=6333
      - LOG_LEVEL=INFO
      - DEFAULT_COLLECTION=promptcraft_knowledge
    depends_on: []
    networks:
      - promptcraft
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # === Context7 Search MCP (NEW) ===
  context7-mcp:
    build:
      context: ./mcp-servers/context7
      dockerfile: Dockerfile
    container_name: context7-mcp
    ports:
      - "8005:8005"
    environment:
      - MCP_PORT=8005
      - MCP_HOST=0.0.0.0
      - CONTEXT7_API_KEY=${CONTEXT7_API_KEY}
      - CONTEXT7_ENDPOINT=https://api.context7.com
      - LOG_LEVEL=INFO
      - RATE_LIMIT_REQUESTS_PER_MINUTE=30
      - CACHE_TTL_SECONDS=300
      - MAX_RESULTS_PER_QUERY=10
    networks:
      - promptcraft
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # === User Interface ===
  gradio-ui:
    build:
      context: ./src/ui
      dockerfile: Dockerfile
    container_name: gradio-ui
    ports:
      - "7860:7860"
    environment:
      - GRADIO_SERVER_PORT=7860
      - ZEN_MCP_ENDPOINT=http://zen-mcp:3000
      - GRADIO_SHARE=false
      - ENABLE_SEARCH_UI=true
      - CONTEXT7_INTEGRATION=enabled
    depends_on:
      zen-mcp-server:
        condition: service_healthy
    volumes:
      - ./src/ui:/app:ro
    networks:
      - promptcraft
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860"]
      interval: 30s
      timeout: 10s
      retries: 3

  # === Knowledge Ingestion ===
  knowledge-webhook:
    build:
      context: ./src/ingestion
      dockerfile: Dockerfile
    container_name: knowledge-webhook
    ports:
      - "5000:5000"
    environment:
      - WEBHOOK_PORT=5000
      - QDRANT_HOST=192.168.1.16
      - QDRANT_PORT=6333
      - GITHUB_WEBHOOK_SECRET=${GITHUB_WEBHOOK_SECRET}
      - LOG_LEVEL=INFO
      - EMBEDDING_MODEL=all-MiniLM-L6-v2
      - BATCH_SIZE=50
    depends_on: []
    volumes:
      - ./knowledge:/app/knowledge:ro
      - ./logs:/app/logs
    networks:
      - promptcraft
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  promptcraft:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  lsp-cache:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      options: size=1g
