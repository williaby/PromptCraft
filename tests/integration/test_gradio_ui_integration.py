"""
Integration tests for Gradio UI compliance with ts-1.md specifications.

This module validates the multi-journey tabbed interface, C.R.E.A.T.E. framework
integration, responsive design, and accessibility compliance for Phase 1 Issue 5.

Test Coverage:
- Multi-journey tabbed interface functionality
- Journey 1 C.R.E.A.T.E. framework integration
- Responsive design across devices
- Accessibility compliance (WCAG 2.1 AA)
- Rate limiting effectiveness
- Session management and isolation
- File upload security validation
- Model selection and cost tracking
"""

from pathlib import Path
import sys
import time
from unittest.mock import MagicMock, patch

import gradio as gr
import pytest


# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from src.config.settings import ApplicationSettings
from src.ui.multi_journey_interface import MultiJourneyInterface, RateLimiter


class TestGradioUIIntegration:
    """Integration tests for Gradio UI compliance and functionality."""

    @pytest.fixture
    def ui_settings(self):
        """Create UI-specific settings for testing."""
        return ApplicationSettings(
            # UI Configuration
            app_name="PromptCraft-Hybrid",
            debug=True,
            # File handling
            max_files=5,
            max_file_size=10 * 1024 * 1024,  # 10MB
            supported_file_types=[".txt", ".md", ".pdf", ".docx", ".csv", ".json"],
            # Performance
            query_timeout=5.0,
            max_concurrent_queries=10,
        )

    @pytest.fixture
    def mock_journey1_processor(self):
        """Mock Journey1SmartTemplates processor for UI testing."""
        mock_processor = MagicMock()

        def mock_enhance_prompt(*args, **kwargs):
            """Mock prompt enhancement with realistic C.R.E.A.T.E. breakdown."""
            return (
                "Enhanced prompt based on C.R.E.A.T.E. framework",  # enhanced_prompt
                "Context: Professional communication with stakeholders",  # context_analysis
                "Request: Generate a professional status update email",  # request_specification
                "Examples: Weekly reports, milestone updates, delay notifications",  # examples_section
                "Augmentations: Professional tone, clear structure, actionable items",  # augmentations_section
                "Tone: Professional, clear, concise communication style",  # tone_format
                "Evaluation: Clear message, appropriate tone, actionable content",  # evaluation_criteria
                '<div class="model-attribution">Generated by: gpt-4o-mini | Time: 1.2s | Cost: $0.003</div>',  # model_attribution
                '<div id="file-sources">No files uploaded</div>',  # file_sources
            )

        mock_processor.enhance_prompt = mock_enhance_prompt
        mock_processor.copy_code_blocks.return_value = "Code blocks copied to clipboard"
        mock_processor.copy_as_markdown.return_value = "Content copied as markdown"

        return mock_processor

    @pytest.fixture
    def mock_export_utils(self):
        """Mock ExportUtils for UI testing."""
        mock_utils = MagicMock()

        def mock_export_journey1_content(*args, **kwargs):
            """Mock content export functionality."""
            return gr.File(value="test_export.md", visible=True)

        mock_utils.export_journey1_content = mock_export_journey1_content
        return mock_utils

    @pytest.fixture
    def multi_journey_interface(self, ui_settings, mock_journey1_processor, mock_export_utils):
        """Create MultiJourneyInterface instance with mocked dependencies."""
        with (
            patch("src.config.settings.get_settings", return_value=ui_settings),
            patch(
                "src.ui.journeys.journey1_smart_templates.Journey1SmartTemplates",
                return_value=mock_journey1_processor,
            ),
            patch("src.ui.components.shared.export_utils.ExportUtils", return_value=mock_export_utils),
        ):
            return MultiJourneyInterface()

    @pytest.mark.integration
    def test_multi_journey_interface_creation(self, multi_journey_interface):
        """Test that multi-journey interface creates successfully with all components."""
        # Test interface initialization
        assert multi_journey_interface is not None
        assert hasattr(multi_journey_interface, "settings")
        assert hasattr(multi_journey_interface, "rate_limiter")
        assert hasattr(multi_journey_interface, "model_costs")

        # Test rate limiter configuration
        assert multi_journey_interface.rate_limiter.max_requests_per_minute == 30
        assert multi_journey_interface.rate_limiter.max_requests_per_hour == 200
        assert multi_journey_interface.rate_limiter.max_file_uploads_per_hour == 50

        # Test model costs are loaded
        assert len(multi_journey_interface.model_costs) > 0
        assert "gpt-4o-mini" in multi_journey_interface.model_costs

    @pytest.mark.integration
    def test_gradio_interface_structure(self, multi_journey_interface):
        """Test that Gradio interface has correct tabbed structure."""
        # Create the interface
        gradio_interface = multi_journey_interface.create_interface()

        # Verify it's a Gradio Blocks interface
        assert isinstance(gradio_interface, gr.Blocks)

        # Test that the interface can be launched (mock the launch)
        with patch.object(gradio_interface, "launch") as mock_launch:
            gradio_interface.launch(server_name="127.0.0.1", server_port=7860, share=False)
            mock_launch.assert_called_once_with(server_name="127.0.0.1", server_port=7860, share=False)

    @pytest.mark.integration
    def test_journey1_create_framework_integration(self, multi_journey_interface):
        """Test Journey 1 C.R.E.A.T.E. framework integration."""
        # Get Journey 1 components for testing
        journey1_components = multi_journey_interface._create_journey1_tab()

        # Verify basic components exist
        assert "input_text" in journey1_components
        assert "enhance_button" in journey1_components
        assert "output_text" in journey1_components

        # Test component types
        assert isinstance(journey1_components["input_text"], gr.Textbox)
        assert isinstance(journey1_components["enhance_button"], gr.Button)
        assert isinstance(journey1_components["output_text"], gr.Textbox)

    @pytest.mark.integration
    def test_file_upload_validation_integration(self, multi_journey_interface):
        """Test file upload validation and security measures."""
        # Test valid file validation
        valid_result = multi_journey_interface.validate_file("test.txt", 1024)  # 1KB file
        assert valid_result[0] is True
        assert "valid" in valid_result[1]

        # Test file size validation (over 10MB limit)
        invalid_size_result = multi_journey_interface.validate_file("large.txt", 50 * 1024 * 1024)  # 50MB
        assert invalid_size_result[0] is False
        assert "too large" in invalid_size_result[1]

        # Test unsupported file type
        invalid_type_result = multi_journey_interface.validate_file("malicious.exe", 1024)
        assert invalid_type_result[0] is False
        assert "Unsupported file type" in invalid_type_result[1]

    @pytest.mark.integration
    def test_rate_limiting_functionality(self, multi_journey_interface):
        """Test rate limiting functionality across different request types."""
        rate_limiter = multi_journey_interface.rate_limiter
        test_session_id = "test_session_123"

        # Test normal request rate limiting
        for _ in range(30):  # Max per minute
            assert rate_limiter.check_request_rate(test_session_id) is True

        # 31st request should be rate limited
        assert rate_limiter.check_request_rate(test_session_id) is False

        # Test file upload rate limiting
        for i in range(50):  # Max per hour
            assert rate_limiter.check_file_upload_rate(f"upload_session_{i}") is True

        # Test rate limit status reporting
        status = rate_limiter.get_rate_limit_status(test_session_id)
        assert "requests_last_minute" in status
        assert "requests_last_hour" in status
        assert "uploads_last_hour" in status
        assert "limits" in status

    @pytest.mark.integration
    def test_session_isolation_and_management(self, multi_journey_interface):
        """Test session isolation and management functionality."""
        # Create multiple sessions
        session1_id = "session_001"
        session2_id = "session_002"

        # Test session state creation and isolation
        state1 = multi_journey_interface.get_session_state(session1_id)
        state2 = multi_journey_interface.get_session_state(session2_id)

        assert state1["session_id"] == session1_id
        assert state2["session_id"] == session2_id
        assert state1 != state2

        # Test session activity updates
        multi_journey_interface.update_session_activity(session1_id)
        updated_state1 = multi_journey_interface.get_session_state(session1_id)
        assert updated_state1["request_count"] == 1

        # Test session cleanup
        multi_journey_interface.cleanup_inactive_sessions(max_age=0)  # Immediate cleanup
        # Should clean up inactive sessions

    @pytest.mark.integration
    def test_model_selection_functionality(self, multi_journey_interface):
        """Test model selection and cost calculation functionality."""
        # Test available models
        available_models = multi_journey_interface.get_available_models()
        assert len(available_models) > 0
        assert "gpt-4o-mini" in available_models

        # Test model selection
        selected_model = multi_journey_interface.select_model("gpt-4o")
        assert selected_model == "gpt-4o"

        # Test invalid model fallback
        fallback_model = multi_journey_interface.select_model("invalid_model")
        assert fallback_model == "gpt-4o-mini"  # Default fallback

        # Test cost calculation
        cost = multi_journey_interface.calculate_cost("gpt-4o-mini", 1000, 500)  # 1K input, 500 output tokens
        assert cost > 0
        assert isinstance(cost, float)

    @pytest.mark.integration
    def test_content_export_functionality(self, multi_journey_interface):
        """Test content export functionality in different formats."""
        test_content = "# Test Content\n\nThis is test content for export."

        # Test text export
        txt_export = multi_journey_interface.export_content(test_content, "txt")
        assert txt_export == test_content

        # Test markdown export
        md_export = multi_journey_interface.export_content(test_content, "md")
        assert "# Exported Content" in md_export
        assert test_content in md_export

        # Test JSON export
        import json

        json_export = multi_journey_interface.export_content(test_content, "json")
        assert "content" in json_export
        # Parse JSON to validate content properly
        parsed_json = json.loads(json_export)
        assert "content" in parsed_json
        assert parsed_json["content"] == test_content

    @pytest.mark.integration
    def test_health_check_functionality(self, multi_journey_interface):
        """Test health check functionality for system monitoring."""
        health_status = multi_journey_interface.health_check()

        # Verify health check structure
        assert "status" in health_status
        assert "components" in health_status
        assert "timestamp" in health_status

        # Test component health checks
        components = health_status["components"]
        assert "rate_limiter" in components
        assert "session_manager" in components
        assert "model_costs" in components

        # Test overall health determination
        assert health_status["status"] in ["healthy", "degraded", "unhealthy"]

    @pytest.mark.integration
    def test_accessibility_compliance_basics(self, multi_journey_interface):
        """Test basic accessibility compliance features."""
        # Create interface for accessibility testing
        gradio_interface = multi_journey_interface.create_interface()

        # Test that interface has proper ARIA labels and structure
        # Note: Full accessibility testing would require browser automation
        # This tests the setup for accessibility features

        # Verify custom CSS includes accessibility considerations
        gradio_interface.css if hasattr(gradio_interface, "css") else ""

        # Test for basic accessibility patterns in the interface structure
        # This is a foundational test - full WCAG 2.1 AA compliance would need browser testing
        assert gradio_interface is not None

    @pytest.mark.integration
    def test_responsive_design_setup(self, multi_journey_interface):
        """Test responsive design setup and configuration."""
        # Create interface
        gradio_interface = multi_journey_interface.create_interface()

        # Test that CSS includes responsive design elements
        # Note: Full responsive testing would require browser automation with different viewport sizes

        # Verify interface creation for responsive testing setup
        assert gradio_interface is not None

        # Test mobile-friendly configuration
        # This tests the foundation for responsive design

    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_journey1_request_handling_integration(self, multi_journey_interface):
        """Test Journey 1 request handling with mocked backend integration."""
        session_id = "integration_test_session"
        test_input = "Create a professional email template"

        # Test request handling
        response = multi_journey_interface.handle_journey1_request(test_input, session_id)

        # Verify response
        assert response is not None
        assert len(response) > 0
        assert "Enhanced prompt" in response

    @pytest.mark.integration
    def test_error_handling_and_graceful_degradation(self, multi_journey_interface):
        """Test error handling and graceful degradation scenarios."""
        # Test with invalid inputs
        session_id = "error_test_session"

        # Test empty input handling
        empty_response = multi_journey_interface.handle_journey1_request("", session_id)
        assert "Error: Invalid input provided" in empty_response

        # Test rate limit exceeded scenario
        # Exhaust rate limit first
        for _ in range(31):  # Exceed 30 requests per minute
            multi_journey_interface.handle_journey1_request("test", session_id)

        # Next request should be rate limited
        rate_limited_response = multi_journey_interface.handle_journey1_request("test", session_id)
        assert "Rate limit exceeded" in rate_limited_response

    @pytest.mark.integration
    def test_security_validation_integration(self, multi_journey_interface, security_test_inputs):
        """Test security validation with potentially malicious inputs."""
        session_id = "security_test_session"

        # Test with various security attack vectors
        for malicious_input in security_test_inputs[:10]:  # Test first 10 to avoid timeout
            try:
                # Convert None to string for testing
                test_input = str(malicious_input) if malicious_input is not None else ""

                # Test that the system handles malicious input gracefully
                response = multi_journey_interface.handle_journey1_request(test_input, session_id)

                # Verify response doesn't contain unprocessed malicious content
                assert response is not None
                assert isinstance(response, str)

                # Check that dangerous patterns are not echoed back unchanged
                if "<script>" in test_input:
                    assert "<script>" not in response or "Enhanced prompt" in response

            except Exception as e:
                # Security errors should be handled gracefully
                assert "Security Error" in str(e) or "Invalid input" in str(e)

    @pytest.mark.integration
    def test_performance_requirements_validation(self, multi_journey_interface):
        """Test that UI meets basic performance requirements."""
        session_id = "performance_test_session"
        test_input = "Generate a comprehensive API documentation template"

        # Test response time requirement (<5s for Journey 1)
        start_time = time.time()
        response = multi_journey_interface.handle_journey1_request(test_input, session_id)
        end_time = time.time()

        response_time = end_time - start_time
        assert response_time < 5.0, f"Response time {response_time:.2f}s exceeds 5s requirement"
        assert response is not None
        assert len(response) > 0

    @pytest.mark.integration
    def test_system_status_and_monitoring(self, multi_journey_interface):
        """Test system status and monitoring capabilities."""
        # Test system status reporting
        system_status = multi_journey_interface.get_system_status()

        assert "active_sessions" in system_status
        assert "total_requests" in system_status
        assert "uptime" in system_status

        # Verify metrics are reasonable
        assert isinstance(system_status["active_sessions"], int)
        assert isinstance(system_status["total_requests"], int)
        assert isinstance(system_status["uptime"], float)

    @pytest.mark.integration
    def test_create_app_factory_function(self, ui_settings):
        """Test the create_app factory function works correctly."""
        from src.ui.multi_journey_interface import create_app

        with patch("src.config.settings.get_settings", return_value=ui_settings):
            app = create_app()

            # Verify app creation
            assert app is not None
            assert isinstance(app, gr.Blocks)

    @pytest.mark.integration
    def test_ui_component_integration_journey_tabs(self, multi_journey_interface):
        """Test integration between different journey tab components."""
        # Test Journey 1 components
        journey1_components = multi_journey_interface._create_journey1_tab()
        assert len(journey1_components) == 3  # input, button, output

        # Test Journey 2 components
        journey2_components = multi_journey_interface._create_journey2_tab()
        assert len(journey2_components) == 3  # search input, button, results

        # Test Journey 3 components
        journey3_components = multi_journey_interface._create_journey3_tab()
        assert len(journey3_components) == 2  # launch button, status

        # Test Journey 4 components
        journey4_components = multi_journey_interface._create_journey4_tab()
        assert len(journey4_components) == 3  # workflow input, toggle, button

    @pytest.mark.integration
    def test_concurrent_user_session_isolation(self, multi_journey_interface):
        """Test that concurrent user sessions are properly isolated."""
        # Simulate multiple concurrent users
        sessions = []
        for i in range(5):
            session_id = f"concurrent_user_{i}"
            sessions.append(session_id)

            # Each user makes requests
            response = multi_journey_interface.handle_journey1_request(f"User {i} test request", session_id)
            assert response is not None

        # Verify session isolation
        for _i, session_id in enumerate(sessions):
            session_state = multi_journey_interface.get_session_state(session_id)
            assert session_state["session_id"] == session_id
            assert session_state["request_count"] >= 1

    @pytest.mark.integration
    def test_ui_fallback_and_error_recovery(self, multi_journey_interface):
        """Test UI fallback mechanisms and error recovery."""
        # Test with mock processing failures
        with patch.object(
            multi_journey_interface,
            "_process_journey1",
            side_effect=Exception("Mock processing failure"),
        ):
            session_id = "fallback_test_session"

            # System should handle processing failures gracefully
            response = multi_journey_interface.handle_journey1_request("test input", session_id)

            # Should get a graceful fallback response, not crash
            assert response is not None
            assert isinstance(response, str)
            assert len(response) > 0


class TestRateLimiterIntegration:
    """Integration tests specifically for RateLimiter functionality."""

    @pytest.fixture
    def rate_limiter(self):
        """Create RateLimiter instance for testing."""
        return RateLimiter(
            max_requests_per_minute=5,  # Lower limits for testing
            max_requests_per_hour=20,
            max_file_uploads_per_hour=10,
        )

    @pytest.mark.integration
    def test_rate_limiter_sliding_window(self, rate_limiter):
        """Test sliding window rate limiting behavior."""
        session_id = "sliding_window_test"

        # Test requests within limits
        for _i in range(5):
            assert rate_limiter.check_request_rate(session_id) is True

        # Exceed minute limit
        assert rate_limiter.check_request_rate(session_id) is False

        # Test sliding window behavior (would need time manipulation in real test)
        # This tests the basic structure

    @pytest.mark.integration
    def test_rate_limiter_cleanup(self, rate_limiter):
        """Test rate limiter cleanup functionality."""
        # Create some sessions
        for i in range(10):
            session_id = f"cleanup_test_{i}"
            rate_limiter.check_request_rate(session_id)

        # Force cleanup
        rate_limiter._cleanup_old_entries()

        # Verify cleanup doesn't break functionality
        assert rate_limiter.check_request_rate("new_session") is True

    @pytest.mark.integration
    def test_rate_limiter_concurrent_sessions(self, rate_limiter):
        """Test rate limiter with concurrent sessions."""
        sessions = [f"concurrent_session_{i}" for i in range(10)]

        # Each session should have independent rate limits
        for session_id in sessions:
            for _ in range(3):  # Under the limit
                assert rate_limiter.check_request_rate(session_id) is True

        # Verify sessions don't interfere with each other
        for session_id in sessions:
            status = rate_limiter.get_rate_limit_status(session_id)
            assert status["requests_last_minute"] == 3


if __name__ == "__main__":
    """
    Run integration tests for Gradio UI functionality.

    Usage:
        python -m pytest tests/integration/test_gradio_ui_integration.py -v
        python tests/integration/test_gradio_ui_integration.py  # Direct execution
    """
    pytest.main([__file__, "-v"])
